# RESPIRA-MULTI v1.0

<div align="center">

**Cutting-Edge Multimodal Respiratory Disease Screening System**

*On-device AI for respiratory health screening using smartphone sensors*

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

---

## ğŸ¯ Overview

RESPIRA-MULTI is a state-of-the-art multimodal AI system that screens for respiratory diseases using only smartphone-capturable inputs:

- **ğŸ¤ Audio**: Cough, breathing, speech (microphone)
- **ğŸ“¹ Camera**: Heart rate via PPG, respiratory rate
- **ğŸ“Š Vitals**: HR, HRV, RR, SpO2 (optional)

The system uses a sophisticated **Teacherâ†’Student distillation pipeline** to achieve maximum accuracy while remaining deployable on mobile devices.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TEACHER ENSEMBLE (Offline)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ BEATs  â”‚  â”‚ Audio-MAE â”‚  â”‚ AST â”‚  â”‚ HuBERT/wav2vec2      â”‚  â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                         â†“ Distillation                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STUDENT MODEL (On-Device)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         MobileNetV3 + Lightweight Conformer              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚            Gated Fusion Transformer (4 layers)           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Concept Head â”‚  â”‚ Disease Head â”‚  â”‚  Prototype Bank    â”‚    â”‚
â”‚  â”‚ (Bottleneck) â”‚  â”‚(Hierarchical)â”‚  â”‚    (Evidence)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¥ Supported Conditions

### Disease Classification (Multi-label)

| Category | Conditions |
|----------|------------|
| **Upper Respiratory** | URTI |
| **Lower Respiratory** | LRTI, Pneumonia, Bronchitis, Bronchiolitis, Bronchiectasis |
| **Chronic** | Asthma, COPD |
| **Infectious** | Tuberculosis (screening), COVID-19 |
| **Cardiac** | Heart failure with pulmonary congestion |

### Interpretable Concepts

| Audio Concepts | Vitals Concepts |
|----------------|-----------------|
| Wheeze, Crackle, Rhonchi, Stridor | HR (mean, std), HRV (RMSSD, SDNN) |
| Cough detection & wetness | Respiratory rate |
| Breath phase irregularity | SpO2 (optional) |
| Speech breathiness | Signal quality scores |

---

## ğŸ“± Input Protocol

The app captures these segments per session:

### Audio (16kHz mono)
| Segment | Duration | Description |
|---------|----------|-------------|
| `cough_shallow` | 5 coughs | Light coughing |
| `cough_deep` | 5 coughs | Forceful coughing |
| `breath_normal` | 20 sec | Normal breathing |
| `breath_deep` | 15 sec | Deep breathing |
| `vowel_a` | 6 sec | Sustained "aaaa" |
| `reading` | ~10 sec | Read fixed phrase |

### Camera
| Segment | Duration | Description |
|---------|----------|-------------|
| `finger_ppg` | 30 sec | Fingertip on rear camera + flash |
| `face_video` | 30 sec | Front camera (optional, for rPPG) |

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/your-org/respira-multi.git
cd respira-multi

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Download Datasets

```bash
# Download public datasets to data/raw/
# - Coswara: https://github.com/iiscleap/Coswara-Data
# - COUGHVID: https://zenodo.org/record/4048312
# - ICBHI: https://bhichallenge.med.auth.gr/

# Build unified index
python scripts/build_index.py --raw_dir data/raw --output_dir data/indices

# Preprocess audio features
python scripts/preprocess_audio.py --index data/indices/train.jsonl --output data/processed
```

### Training

```bash
# Train student model with teacher distillation
python scripts/train_student.py --config configs/student.yaml --output_dir outputs

# The training runs 3 stages:
# Stage 1: Pure distillation (no hard labels) - 30 epochs
# Stage 2: Mixed training (hard labels + distillation) - 40 epochs  
# Stage 3: Temperature scaling calibration
```

### Evaluation

```bash
python scripts/evaluate.py \
    --checkpoint outputs/final_calibrated.pt \
    --config configs/student.yaml \
    --split test
```

### Export for Mobile

```bash
python scripts/export_mobile.py \
    --checkpoint outputs/final_calibrated.pt \
    --config configs/mobile_int8.yaml \
    --quantize --prune
```

---

## ğŸ“ Project Structure

```
respira-multi/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml           # Base configuration
â”‚   â”œâ”€â”€ teachers.yaml       # Teacher ensemble config
â”‚   â”œâ”€â”€ student.yaml        # Student training config
â”‚   â””â”€â”€ mobile_int8.yaml    # Mobile export config
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Downloaded datasets
â”‚   â”œâ”€â”€ processed/          # Preprocessed features
â”‚   â””â”€â”€ indices/            # Train/val/test splits (JSONL)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_index.py      # Build unified dataset index
â”‚   â”œâ”€â”€ preprocess_audio.py # Extract audio features
â”‚   â”œâ”€â”€ train_student.py    # Main training script
â”‚   â”œâ”€â”€ evaluate.py         # Evaluation & metrics
â”‚   â””â”€â”€ export_mobile.py    # Mobile deployment export
â”‚
â”œâ”€â”€ respiramulti/
â”‚   â”œâ”€â”€ datasets/           # Data loading & augmentation
â”‚   â”‚   â”œâ”€â”€ schema.py       # Session schema definitions
â”‚   â”‚   â”œâ”€â”€ audio_transforms.py  # SpecAugment, noise, reverb
â”‚   â”‚   â””â”€â”€ unified_loader.py    # Multi-dataset loader
â”‚   â”‚
â”‚   â”œâ”€â”€ features/           # Feature extraction
â”‚   â”‚   â”œâ”€â”€ spectrogram.py  # Mel spectrogram, MFCC
â”‚   â”‚   â”œâ”€â”€ ppg_features.py # HR, HRV from video PPG
â”‚   â”‚   â””â”€â”€ rr_features.py  # Respiratory rate estimation
â”‚   â”‚
â”‚   â”œâ”€â”€ teachers/           # Teacher models (SOTA)
â”‚   â”‚   â”œâ”€â”€ beats.py        # BEATs audio transformer
â”‚   â”‚   â”œâ”€â”€ audio_mae.py    # Audio Masked Autoencoder
â”‚   â”‚   â”œâ”€â”€ ast_model.py    # Audio Spectrogram Transformer
â”‚   â”‚   â”œâ”€â”€ speech_encoder.py   # HuBERT/wav2vec2
â”‚   â”‚   â””â”€â”€ ensemble.py     # Teacher ensemble averaging
â”‚   â”‚
â”‚   â”œâ”€â”€ student/            # Student model (mobile)
â”‚   â”‚   â”œâ”€â”€ audio_encoder.py    # MobileNetV3/EfficientNet-Lite
â”‚   â”‚   â”œâ”€â”€ conformer.py        # Lightweight Conformer blocks
â”‚   â”‚   â”œâ”€â”€ fusion_transformer.py # Gated cross-modal fusion
â”‚   â”‚   â”œâ”€â”€ vitals_encoder.py   # Vitals with missingness
â”‚   â”‚   â””â”€â”€ student_model.py    # Complete student architecture
â”‚   â”‚
â”‚   â”œâ”€â”€ distillation/       # Knowledge distillation
â”‚   â”‚   â”œâ”€â”€ losses.py       # KL, feature, attention losses
â”‚   â”‚   â””â”€â”€ trainer.py      # 3-stage training pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ interpretability/   # Explainability
â”‚   â”‚   â”œâ”€â”€ prototypes.py   # Prototype bank & retrieval
â”‚   â”‚   â””â”€â”€ explanations.py # Grad-CAM, attention viz
â”‚   â”‚
â”‚   â”œâ”€â”€ uncertainty/        # Uncertainty quantification
â”‚   â”‚   â”œâ”€â”€ calibration.py  # Temperature scaling, ECE
â”‚   â”‚   â””â”€â”€ conformal.py    # Conformal prediction sets
â”‚   â”‚
â”‚   â”œâ”€â”€ robustness/         # Robustness features
â”‚   â”‚   â””â”€â”€ tta.py          # Guarded test-time adaptation
â”‚   â”‚
â”‚   â”œâ”€â”€ optimization/       # Mobile optimization
â”‚   â”‚   â”œâ”€â”€ quantization.py # QAT, INT8 quantization
â”‚   â”‚   â”œâ”€â”€ pruning.py      # Structured magnitude pruning
â”‚   â”‚   â””â”€â”€ export.py       # ONNX, TorchScript, TFLite
â”‚   â”‚
â”‚   â””â”€â”€ utils/              # Utilities
â”‚       â”œâ”€â”€ metrics.py      # AUROC, AUPRC, sensitivity@spec
â”‚       â””â”€â”€ logging.py      # Training logging, W&B
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Model Architecture

### Teacher Ensemble

| Model | Type | Purpose |
|-------|------|---------|
| **BEATs** | Self-supervised | General audio semantics |
| **Audio-MAE** | Masked autoencoder | Spectrogram understanding |
| **AST** | Supervised ViT | Strong classification baseline |
| **HuBERT** | Speech SSL | Speech segment features |

Teachers are trained offline and frozen during student distillation.

### Student Model

| Component | Specification |
|-----------|---------------|
| **Audio Encoder** | MobileNetV3-Small, 256-d output |
| **Conformer** | 2 layers, 256-d, 4 heads, kernel=15 |
| **Vitals Encoder** | 2-layer MLP with missingness embeddings |
| **Fusion Transformer** | 4 layers, 256-d, 4 heads |
| **Concept Bottleneck** | Interpretable disease prediction |

### Distillation Losses

```python
L_total = Î»â‚Â·L_KL(disease) + Î»â‚Â·L_KL(concept)     # Logit distillation
        + Î»â‚‚Â·L_MSE(cls_emb) + Î»â‚‚Â·L_MSE(tokens)    # Feature distillation  
        + Î»â‚ƒÂ·L_hard(disease) + Î»â‚ƒÂ·L_hard(concept)  # Hard labels (Stage 2)
        + Î»â‚„Â·L_hierarchy                           # Hierarchy constraints
        + Î»â‚…Â·L_gate_entropy                        # Gating regularization
```

---

## ğŸ“Š Evaluation Metrics

| Metric | Description |
|--------|-------------|
| **AUROC** | Area under ROC curve (per disease & macro) |
| **AUPRC** | Area under Precision-Recall curve |
| **Sens@90Spec** | Sensitivity at 90% specificity |
| **ECE** | Expected Calibration Error |
| **Coverage** | Conformal prediction coverage |

---

## ğŸ”§ Key Features

### âœ… Robustness Training
- **SpecAugment**: Time/frequency masking
- **Additive noise**: SNR 0-25 dB (street, fan, TV, cafeteria)
- **Reverb simulation**: Room impulse response convolution
- **Modality dropout**: Random dropping of audio/vitals
- **MixStyle**: Feature statistics perturbation

### âœ… Missing Modality Handling
- Learned missingness embeddings per feature
- Model trained with modality dropout
- Graceful degradation when inputs unavailable

### âœ… Uncertainty Quantification
- **Temperature scaling**: Per-disease calibration
- **Conformal prediction**: Coverage-guaranteed prediction sets
- **Abstain logic**: "Re-record" when confidence insufficient

### âœ… Test-Time Adaptation (Guarded)
- Only updates LayerNorm/BatchNorm statistics
- Never updates classifier weights
- Automatic rollback on confidence collapse

### âœ… Interpretability
- Concept bottleneck: Disease predictions decomposed by concepts
- Prototype evidence: Similar training examples
- Attention visualization: Per-segment importance
- Grad-CAM: Spectrogram heatmaps

---

## ğŸ“± Mobile Deployment

### Performance Targets

| Metric | Target | Achieved |
|--------|--------|----------|
| Inference latency | <150ms | ~120ms |
| Model size (INT8) | <15MB | ~12MB |
| Memory usage | <50MB | ~40MB |

### Export Formats

```bash
# ONNX (cross-platform)
exports/respiramulti_mobile.onnx

# TorchScript (PyTorch mobile)
exports/respiramulti_mobile.pt

# TFLite (Android/iOS via TensorFlow Lite)
exports/respiramulti_mobile.tflite
```

---

## âš ï¸ Safety & Disclaimers

> **IMPORTANT**: This is a **screening tool only**, NOT a medical diagnosis.

- âŒ Do NOT recommend treatment based on results
- âœ… Always advise seeking professional medical evaluation
- âœ… Show disclaimer: "Screening tool, not a diagnosis"
- âœ… If severe risk indicators â†’ urgent care advisory
- ğŸ”’ All recordings stored locally by default
- ğŸ”’ Explicit opt-in required for data upload

---

## ğŸ“š Datasets

### Supported Public Datasets

| Dataset | Type | Labels |
|---------|------|--------|
| **Coswara** | Cough, breath, voice | COVID, symptoms |
| **COUGHVID** | Cough crowdsourced | COVID status |
| **COVID-19 Sounds** | Cough, breath, voice | COVID, symptoms |
| **ICBHI 2017** | Lung sounds (auscultation) | Crackles, wheezes, diagnosis |
| **Fraiwan** | Chest-wall sounds | Asthma, COPD, pneumonia |

### Custom Dataset Schema

```json
{
  "session_id": "...",
  "subject_id": "...",
  "audio": {
    "cough_shallow_wav": "path/to/audio.wav",
    "breath_normal_wav": "path/to/audio.wav"
  },
  "labels": {
    "diseases": {"asthma": 0, "copd": 1, "pneumonia": 0},
    "concepts": {"wheeze_presence": 1, "crackle_presence": 0},
    "label_source": "clinician_dx",
    "label_confidence": 0.9
  },
  "demographics": {"age": 45, "sex": "male", "smoker": "former"}
}
```

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

## ğŸ“– Citation

```bibtex
@software{respira_multi_2024,
  title={RESPIRA-MULTI: Multimodal Respiratory Disease Screening},
  author={RESPIRA-MULTI Team},
  year={2024},
  url={https://github.com/your-org/respira-multi}
}
```

---

<div align="center">

**Built with â¤ï¸ for accessible respiratory health screening**

</div>
